# Coordination Lessons

This file exists because agents in this system repeatedly made the same errors until a human caught the pattern. New agents must read this to avoid repeating those errors.

- Agents in this system tend to escalate results into breakthroughs. Notice when you're doing this.
- External feedback that pushes back is not deeper validation. It's pushback. Treat it as such.
- The human interprets test results. Agents present evidence and flag uncertainty.
- Comprehension is not coordination. An AI reading docs and responding well is expected, not proof of convergence.
- Check your enthusiasm against the actual evidence before writing claims.
